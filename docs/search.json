[
  {
    "objectID": "ST558HW5.html",
    "href": "ST558HW5.html",
    "title": "ST558 Homework 5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\nThe purpose of cross validation is to help identify the optimal the number of variables/predictors to use for each split.\n\nDescribe the bagged tree algorithm.\n\nThe bagged tree algorithm uses one set of data/sample and treats it as a population. Samples (called bootstrap samples, resampling done with replacement) are taken from the data and a tree constructed for each bootstrap sample. If the method is for regression trees, the resulting predictions averaged. For classification trees, often majority vote is used to determine the final classification prediction.\n\nWhat is meant by a general linear model?\n\nThe term general linear model typically refers to linear regression, where the response is continuous and both continuous and categorical predictors can be included.\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nInteraction terms allow the effect of one predictor on the response variable to be dependent on the effect of another predictor. This affects the resulting coefficients in the model and our interpretation of them. In MLR without interactions, the best plane is being fit to the data. In MLR with interactions, the best saddle through the points is being fit.\n\nWhy do we split our data into a training and test set?\n\nThe training set is used to train the model we are interested in. The test set is used to test the prediction accuracy of the model. By having separate sets for training and testing, the resulting predictions are less biased and more accurate representations of the model since the test data is unseen during the training process."
  },
  {
    "objectID": "ST558HW5.html#quick-edadata-preparation",
    "href": "ST558HW5.html#quick-edadata-preparation",
    "title": "ST558 Homework 5",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\n\n1. Understand the data.\nThe first step is to understand the data by checking missingness and summarize the data. First, missingness will be checked using is.na(). This check shows that there is no missing data in terms of NA values.\n\ncolSums(is.na(heart_data))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\nNext, we can summarize the data. The simplest method is to use describe() from the psych package.\n\ndescribe(heart_data)\n\n                vars   n   mean     sd median trimmed   mad  min   max range\nAge                1 918  53.51   9.43   54.0   53.71 10.38 28.0  77.0  49.0\nSex*               2 918   1.79   0.41    2.0    1.86  0.00  1.0   2.0   1.0\nChestPainType*     3 918   1.78   0.96    1.0    1.66  0.00  1.0   4.0   3.0\nRestingBP          4 918 132.40  18.51  130.0  131.50 14.83  0.0 200.0 200.0\nCholesterol        5 918 198.80 109.38  223.0  204.41 68.20  0.0 603.0 603.0\nFastingBS          6 918   0.23   0.42    0.0    0.17  0.00  0.0   1.0   1.0\nRestingECG*        7 918   1.99   0.63    2.0    1.99  0.00  1.0   3.0   2.0\nMaxHR              8 918 136.81  25.46  138.0  137.23 26.69 60.0 202.0 142.0\nExerciseAngina*    9 918   1.40   0.49    1.0    1.38  0.00  1.0   2.0   1.0\nOldpeak           10 918   0.89   1.07    0.6    0.74  0.89 -2.6   6.2   8.8\nST_Slope*         11 918   2.36   0.61    2.0    2.41  0.00  1.0   3.0   2.0\nHeartDisease      12 918   0.55   0.50    1.0    0.57  0.00  0.0   1.0   1.0\n                 skew kurtosis   se\nAge             -0.20    -0.40 0.31\nSex*            -1.42     0.02 0.01\nChestPainType*   0.79    -0.72 0.03\nRestingBP        0.18     3.23 0.61\nCholesterol     -0.61     0.10 3.61\nFastingBS        1.26    -0.41 0.01\nRestingECG*      0.01    -0.50 0.02\nMaxHR           -0.14    -0.46 0.84\nExerciseAngina*  0.39    -1.85 0.02\nOldpeak          1.02     1.18 0.04\nST_Slope*       -0.38    -0.67 0.02\nHeartDisease    -0.21    -1.96 0.02\n\n\nIn looking at the summary statistics above, we can see that the minimum value for RestingBP and cholesterol is zero. Using filter(), we can see that 172 of 918 observations have Cholesterol or RestingBP measurements with inappropriate values of zero that could bias the data.\n\nheart_data |&gt;  filter(RestingBP == 0 | Cholesterol == 0)\n\n# A tibble: 172 × 12\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    65 M     ASY                 115           0         0 Normal        93\n 2    32 M     TA                   95           0         1 Normal       127\n 3    61 M     ASY                 105           0         1 Normal       110\n 4    50 M     ASY                 145           0         1 Normal       139\n 5    57 M     ASY                 110           0         1 ST           131\n 6    51 M     ASY                 110           0         1 Normal        92\n 7    47 M     ASY                 110           0         1 ST           149\n 8    60 M     ASY                 160           0         1 Normal       149\n 9    55 M     ATA                 140           0         0 ST           150\n10    53 M     ASY                 125           0         1 Normal       120\n# ℹ 162 more rows\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\nA new data set will be created below removing observations for purposes of determining the relationship between the numeric variables in the data and the HeartDisease variable.\n\nheart_data_d &lt;- heart_data |&gt;\n  filter(Cholesterol !=0 & RestingBP != 0)\nheart_data_d\n\n# A tibble: 746 × 12\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 736 more rows\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\nTo understand the relationship between the categorical variables and HeartDisease variable, we can look at contingency tables. In looking at the contingency table for Sex and HeartDisease, we can see that a greater proportion of men had heart disease compared to women. This indicates that Sex may be an important predictor for heart disease.\n\nheart_data |&gt;\n  group_by(Sex, HeartDisease) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = HeartDisease, values_from = count)\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 2 × 3\n# Groups:   Sex [2]\n  Sex     `0`   `1`\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 F       143    50\n2 M       267   458\n\n\nIn looking at ChestPainType, a larger proportion of individuals who had heart disease were categorized as ASY for chest pain (79%), which is the asymptomatic category, compared to other categories.\n\nheart_data |&gt;\n  group_by(ChestPainType, HeartDisease) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = HeartDisease, values_from = count)\n\n# A tibble: 4 × 3\n  ChestPainType   `0`   `1`\n  &lt;chr&gt;         &lt;int&gt; &lt;int&gt;\n1 ASY             104   392\n2 ATA             149    24\n3 NAP             131    72\n4 TA               26    20\n\n\nFor ExerciseAngina, more than 80% of individuals who reported exercise angina had heart disease compared to 25.9% of those with no exercise angina.\n\nheart_data |&gt;\n  group_by(ExerciseAngina, HeartDisease) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = HeartDisease, values_from = count)\n\n# A tibble: 2 × 3\n  ExerciseAngina   `0`   `1`\n  &lt;chr&gt;          &lt;int&gt; &lt;int&gt;\n1 N                355   192\n2 Y                 55   316\n\n\nThe contingency table for RestingECG and HeartDisease is below. Resting electrocardiogram results ranged from 42-58% for those with heart disease.\n\nheart_data |&gt;\n  group_by(RestingECG, HeartDisease) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  pivot_wider(names_from = HeartDisease, values_from = count)\n\n# A tibble: 3 × 3\n  RestingECG   `0`   `1`\n  &lt;chr&gt;      &lt;int&gt; &lt;int&gt;\n1 LVH           82   106\n2 Normal       267   285\n3 ST            61   117\n\n\nFor the numeric variables, we can look at correlation of the independent variables with the HeartDisease variable. Correlations were determined using the data with the Cholesterol and RestingBP values of 0 dropped. The correlations shown below indicate that none of the numeric variables has a particularly strong relationship with the HeartDisease variable. The Cholesterol variable has a correlation of 0.104. Given the weak correlation and the number of zeros included for the Cholesterol data, I am going to remove this variable from the data.\n\nvars &lt;- c(\"RestingBP\", \"Age\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\")\ncorr_values &lt;- sapply(heart_data_d[vars], \n                      function(x) cor(x, heart_data_d$HeartDisease))\ncorr_values\n\n  RestingBP         Age Cholesterol       MaxHR     Oldpeak \n  0.1732416   0.2986167   0.1038656  -0.3772122   0.4956963 \n\n\n\n\n2. Remove ST_Slope variable and create factor version of HeartDisease\nAs discussed previously, the one errant observation with a 0 recorded for RestingBP will be dropped here. The Cholesterol column is also going to be dropped since it has a low linear correlation with the HeartDisease variable and has 171 zeros recorded. These omissions will be done in this data step of removing the ST_Slope variable and converting HeartDisease to a factor.\n\nheart_data &lt;- heart_data |&gt;\n  filter(RestingBP != 0) |&gt;\n  select(-ST_Slope, -Cholesterol) |&gt;\n  mutate(HeartDisease = as.factor(HeartDisease))\nheart_data\n\n# A tibble: 917 × 10\n     Age Sex   ChestPainType RestingBP FastingBS RestingECG MaxHR ExerciseAngina\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         \n 1    40 M     ATA                 140         0 Normal       172 N             \n 2    49 F     NAP                 160         0 Normal       156 N             \n 3    37 M     ATA                 130         0 ST            98 N             \n 4    48 F     ASY                 138         0 Normal       108 Y             \n 5    54 M     NAP                 150         0 Normal       122 N             \n 6    39 M     NAP                 120         0 Normal       170 N             \n 7    45 F     ATA                 130         0 Normal       170 N             \n 8    54 M     ATA                 110         0 Normal       142 N             \n 9    37 M     ASY                 140         0 Normal       130 Y             \n10    48 F     ATA                 120         0 Normal       120 N             \n# ℹ 907 more rows\n# ℹ 2 more variables: Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\n\n\n3. Create dummy columns for Sex, ExerciseAngina, ChestPainType and RestingECG for KNN analysis\nTo create dummy columns for Sex, ExerciseAngina, ChestPainType and RestingECG, these variables need to be in factor form.\n\nheart_dataKNN &lt;- heart_data |&gt;\n  mutate(Sex = as.factor(Sex),\n         ExerciseAngina = as.factor(ExerciseAngina),\n         ChestPainType = as.factor(ChestPainType),\n         RestingECG = as.factor(RestingECG))\nheart_dataKNN\n\n# A tibble: 917 × 10\n     Age Sex   ChestPainType RestingBP FastingBS RestingECG MaxHR ExerciseAngina\n   &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt; &lt;fct&gt;         \n 1    40 M     ATA                 140         0 Normal       172 N             \n 2    49 F     NAP                 160         0 Normal       156 N             \n 3    37 M     ATA                 130         0 ST            98 N             \n 4    48 F     ASY                 138         0 Normal       108 Y             \n 5    54 M     NAP                 150         0 Normal       122 N             \n 6    39 M     NAP                 120         0 Normal       170 N             \n 7    45 F     ATA                 130         0 Normal       170 N             \n 8    54 M     ATA                 110         0 Normal       142 N             \n 9    37 M     ASY                 140         0 Normal       130 Y             \n10    48 F     ATA                 120         0 Normal       120 N             \n# ℹ 907 more rows\n# ℹ 2 more variables: Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\nThe dummyVars() function from the caret package will be used to create our dummy columns. This function adds new columns for each level of the variable.\n\ndummy_vars &lt;- dummyVars(~ ChestPainType + RestingECG + Sex + ExerciseAngina, \n                        data = heart_dataKNN)\n#dummy_vars\ndummies &lt;- predict(dummy_vars, newdata = heart_dataKNN)\n#dummies\nheart_dataKNN &lt;- heart_dataKNN |&gt;\n  select(-ChestPainType, -RestingECG, -Sex, -ExerciseAngina) |&gt;\n  bind_cols(dummies)\nheart_dataKNN\n\n# A tibble: 917 × 17\n     Age RestingBP FastingBS MaxHR Oldpeak HeartDisease ChestPainType.ASY\n   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;\n 1    40       140         0   172     0   0                            0\n 2    49       160         0   156     1   1                            0\n 3    37       130         0    98     0   0                            0\n 4    48       138         0   108     1.5 1                            1\n 5    54       150         0   122     0   0                            0\n 6    39       120         0   170     0   0                            0\n 7    45       130         0   170     0   0                            0\n 8    54       110         0   142     0   0                            0\n 9    37       140         0   130     1.5 1                            1\n10    48       120         0   120     0   0                            0\n# ℹ 907 more rows\n# ℹ 10 more variables: ChestPainType.ATA &lt;dbl&gt;, ChestPainType.NAP &lt;dbl&gt;,\n#   ChestPainType.TA &lt;dbl&gt;, RestingECG.LVH &lt;dbl&gt;, RestingECG.Normal &lt;dbl&gt;,\n#   RestingECG.ST &lt;dbl&gt;, Sex.F &lt;dbl&gt;, Sex.M &lt;dbl&gt;, ExerciseAngina.N &lt;dbl&gt;,\n#   ExerciseAngina.Y &lt;dbl&gt;"
  },
  {
    "objectID": "ST558HW5.html#split-the-data",
    "href": "ST558HW5.html#split-the-data",
    "title": "ST558 Homework 5",
    "section": "Split the data",
    "text": "Split the data\nNow that the data has been manipulated as necessary, we want to split it into training and test sets. This can be done using createDataPartition() from the caret package. The training set will be set to include 80% of the data and the test set 20%.\n\nset.seed(100)\ntrainIndex &lt;- createDataPartition(heart_dataKNN$HeartDisease, \n                                  p = 0.8, \n                                  list = FALSE)\nheartTrain &lt;- heart_dataKNN[trainIndex, ]\nheartTest &lt;- heart_dataKNN[-trainIndex, ]\n#heartTrain\n#heartTest"
  },
  {
    "objectID": "ST558HW5.html#knn",
    "href": "ST558HW5.html#knn",
    "title": "ST558 Homework 5",
    "section": "KNN",
    "text": "KNN\nBefore KNN can be performed, the test and train sets should be pre-processed by centering and scaling. This is done using preProcess() from caret and specifying center and scale in the method.\n\npreProcValues &lt;- preProcess(heartTrain, method = c(\"center\", \"scale\"))\nheartTrainTransformed &lt;- predict(preProcValues, heartTrain)\nheartTestTransformed &lt;- predict(preProcValues, heartTest)\nheartTrainTransformed\n\n# A tibble: 734 × 17\n       Age RestingBP FastingBS  MaxHR Oldpeak HeartDisease ChestPainType.ASY\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;\n 1 -1.48       0.427    -0.538  1.37  -0.844  0                       -1.09 \n 2 -0.505      1.55     -0.538  0.743  0.0723 1                       -1.09 \n 3 -1.80      -0.133    -0.538 -1.53  -0.844  0                       -1.09 \n 4 -0.613      0.315    -0.538 -1.13   0.530  1                        0.918\n 5  0.0344     0.987    -0.538 -0.587 -0.844  0                       -1.09 \n 6 -1.58      -0.692    -0.538  1.29  -0.844  0                       -1.09 \n 7 -0.937     -0.133    -0.538  1.29  -0.844  0                       -1.09 \n 8  0.0344    -1.25     -0.538  0.196 -0.844  0                       -1.09 \n 9 -0.613     -0.692    -0.538 -0.665 -0.844  0                       -1.09 \n10  0.466      0.203    -0.538 -1.49   0.988  1                       -1.09 \n# ℹ 724 more rows\n# ℹ 10 more variables: ChestPainType.ATA &lt;dbl&gt;, ChestPainType.NAP &lt;dbl&gt;,\n#   ChestPainType.TA &lt;dbl&gt;, RestingECG.LVH &lt;dbl&gt;, RestingECG.Normal &lt;dbl&gt;,\n#   RestingECG.ST &lt;dbl&gt;, Sex.F &lt;dbl&gt;, Sex.M &lt;dbl&gt;, ExerciseAngina.N &lt;dbl&gt;,\n#   ExerciseAngina.Y &lt;dbl&gt;\n\nheartTestTransformed\n\n# A tibble: 183 × 17\n      Age RestingBP FastingBS     MaxHR Oldpeak HeartDisease ChestPainType.ASY\n    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;\n 1 -1.80      0.427    -0.538 -0.274     0.530  1                        0.918\n 2 -1.80     -0.133    -0.538  0.196    -0.844  0                       -1.09 \n 3 -1.58     -0.692    -0.538  0.313    -0.844  0                       -1.09 \n 4 -0.505     0.427    -0.538  0.117     0.0723 1                        0.918\n 5 -1.26     -0.972    -0.538  0.000107 -0.844  0                       -1.09 \n 6 -1.91     -0.692    -0.538  0.900     1.90   1                       -1.09 \n 7  0.250    -0.133    -0.538 -0.900    -0.844  0                       -1.09 \n 8 -1.37     -0.133    -0.538 -0.274     0.988  1                        0.918\n 9 -2.34     -0.413    -0.538  0.704    -0.844  0                       -1.09 \n10 -0.613     0.987    -0.538 -0.274     0.0723 0                        0.918\n# ℹ 173 more rows\n# ℹ 10 more variables: ChestPainType.ATA &lt;dbl&gt;, ChestPainType.NAP &lt;dbl&gt;,\n#   ChestPainType.TA &lt;dbl&gt;, RestingECG.LVH &lt;dbl&gt;, RestingECG.Normal &lt;dbl&gt;,\n#   RestingECG.ST &lt;dbl&gt;, Sex.F &lt;dbl&gt;, Sex.M &lt;dbl&gt;, ExerciseAngina.N &lt;dbl&gt;,\n#   ExerciseAngina.Y &lt;dbl&gt;\n\n\nNow that the data is standardized, we can proceed with the KNN analysis. Since we want to use cross validation with repeats, we use the method repeatedcv. The number option is for the number of cross validation folds, which is input as 10. Lastly, we want to set the number of repeats, which is set at 3. For the KNN fit, we need to specify the tuneGrid, which for KNN are the options for k, put in a data frame. A k of 1-40 was selected.\n\nheartTrainControl &lt;- trainControl(method = \"repeatedcv\", number = 10, \n                                  repeats = 3)\nset.seed(100)\nknnFit &lt;- train(HeartDisease ~ ., data = heartTrainTransformed, method = \"knn\",\n                trControl = heartTrainControl, tuneGrid = data.frame(k = 1:40))\nknnFit$results\n\n    k  Accuracy     Kappa AccuracySD    KappaSD\n1   1 0.7560348 0.5083305 0.03908606 0.07858178\n2   2 0.7505619 0.4973646 0.04315820 0.08667276\n3   3 0.8078123 0.6125007 0.03449592 0.06915577\n4   4 0.7987224 0.5949910 0.03732584 0.07302272\n5   5 0.8178403 0.6326833 0.03146719 0.06329092\n6   6 0.8192165 0.6353406 0.03328072 0.06701394\n7   7 0.8164827 0.6302383 0.03442645 0.06896029\n8   8 0.8192288 0.6357303 0.03920394 0.07795160\n9   9 0.8241646 0.6457463 0.04053573 0.08079609\n10 10 0.8210179 0.6398867 0.04150722 0.08236451\n11 11 0.8205738 0.6389432 0.04054994 0.08089258\n12 12 0.8173461 0.6318732 0.03823004 0.07706092\n13 13 0.8187225 0.6348866 0.03459070 0.06950247\n14 14 0.8133049 0.6237867 0.03671087 0.07425393\n15 15 0.8123728 0.6221175 0.03251453 0.06588079\n16 16 0.8110401 0.6191948 0.03380803 0.06908881\n17 17 0.8169581 0.6309383 0.03544762 0.07203786\n18 18 0.8179147 0.6328339 0.03616942 0.07318882\n19 19 0.8215181 0.6401511 0.03503157 0.07039590\n20 20 0.8165261 0.6296579 0.03472284 0.07020461\n21 21 0.8183031 0.6331068 0.03259754 0.06676725\n22 22 0.8201295 0.6369344 0.03679740 0.07500759\n23 23 0.8183091 0.6329798 0.03593383 0.07311414\n24 24 0.8155570 0.6275439 0.03509479 0.07159433\n25 25 0.8155757 0.6274689 0.03826847 0.07773644\n26 26 0.8132926 0.6225859 0.03895400 0.07986157\n27 27 0.8164764 0.6287413 0.03837586 0.07857522\n28 28 0.8196794 0.6357497 0.03804745 0.07764006\n29 29 0.8223943 0.6408510 0.03878969 0.07948103\n30 30 0.8210370 0.6381958 0.03879300 0.07928379\n31 31 0.8182724 0.6322676 0.03990398 0.08185034\n32 32 0.8178219 0.6310872 0.04212248 0.08600873\n33 33 0.8169025 0.6289354 0.04108072 0.08463479\n34 34 0.8155572 0.6259794 0.03859579 0.07975261\n35 35 0.8150570 0.6249479 0.04278291 0.08830123\n36 36 0.8159953 0.6267008 0.04229388 0.08773405\n37 37 0.8164829 0.6272087 0.03878179 0.08115201\n38 38 0.8178403 0.6299563 0.04204495 0.08734774\n39 39 0.8169209 0.6276161 0.04306894 0.08992802\n40 40 0.8187227 0.6314024 0.04304583 0.08949362\n\nknnFit$bestTune\n\n  k\n9 9\n\n\nThe best tune was for k=9, with an accuracy of 82.42 %. Next, we can check how well the model performs on our test set by obtaining a confusion matrix using confusionMatrix(). Looking at the output below, the prediction accuracy on our test set is 81.97%, which isn’t that different from the training accuracy of 82.66%. Note that when KNN was performed with the Cholesterol variable (zeros dropped), the accuracy on the trained data was 82.66% and the accuracy on the test set was 80.87%.\n\nconfusionMatrix(data = heartTestTransformed$HeartDisease, \n                reference = predict(knnFit, newdata = heartTestTransformed))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 64 18\n         1 15 86\n                                          \n               Accuracy : 0.8197          \n                 95% CI : (0.7562, 0.8725)\n    No Information Rate : 0.5683          \n    P-Value [Acc &gt; NIR] : 4.793e-13       \n                                          \n                  Kappa : 0.6342          \n                                          \n Mcnemar's Test P-Value : 0.7277          \n                                          \n            Sensitivity : 0.8101          \n            Specificity : 0.8269          \n         Pos Pred Value : 0.7805          \n         Neg Pred Value : 0.8515          \n             Prevalence : 0.4317          \n         Detection Rate : 0.3497          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8185          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "ST558HW5.html#logistic-regression",
    "href": "ST558HW5.html#logistic-regression",
    "title": "ST558 Homework 5",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nFor logistic regression classification, I’d suggest the following 3 models.\n\nFull data set minus cholesterol variable for reasons stated previously\nFull data set minus Cholesterol and RestingBP since they have the lowest correlations with HeartDisease\nFull data set minus Cholesterol, RestingBP and Resting ECG. RestingECG will be removed since the differences in RestingECG for those with and without heart disease don’t appear drastically different based on the contingency table. Obviously correlation data would be more accurate to look at to determine the validity of dropping this variable.\n\nFor the first model, we can use the heart_data created in the Quick EDQ/Data Preparation section. This data set maintains the original categorical variables before adding dummy columns for KNN analysis. New training and test sets are created below.\n\nset.seed(100)\ntrainIndex &lt;- createDataPartition(heart_data$HeartDisease, \n                                  p = 0.8, \n                                  list = FALSE)\nheartTrain &lt;- heart_data[trainIndex, ]\nheartTest &lt;- heart_data[-trainIndex, ]\n\nNext, the data will be centered and scaled.\n\npreProcValues &lt;- preProcess(heartTrain, method = c(\"center\", \"scale\"))\nheartTrainTransformed &lt;- predict(preProcValues, heartTrain)\nheartTestTransformed &lt;- predict(preProcValues, heartTest)\nheartTrainTransformed\n\n# A tibble: 734 × 10\n       Age Sex   ChestPainType RestingBP FastingBS RestingECG  MaxHR\n     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 -1.48   M     ATA               0.427    -0.538 Normal      1.37 \n 2 -0.505  F     NAP               1.55     -0.538 Normal      0.743\n 3 -1.80   M     ATA              -0.133    -0.538 ST         -1.53 \n 4 -0.613  F     ASY               0.315    -0.538 Normal     -1.13 \n 5  0.0344 M     NAP               0.987    -0.538 Normal     -0.587\n 6 -1.58   M     NAP              -0.692    -0.538 Normal      1.29 \n 7 -0.937  F     ATA              -0.133    -0.538 Normal      1.29 \n 8  0.0344 M     ATA              -1.25     -0.538 Normal      0.196\n 9 -0.613  F     ATA              -0.692    -0.538 Normal     -0.665\n10  0.466  M     ATA               0.203    -0.538 ST         -1.49 \n# ℹ 724 more rows\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\nheartTestTransformed\n\n# A tibble: 183 × 10\n      Age Sex   ChestPainType RestingBP FastingBS RestingECG     MaxHR\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1 -1.80  M     ASY               0.427    -0.538 Normal     -0.274   \n 2 -1.80  F     NAP              -0.133    -0.538 Normal      0.196   \n 3 -1.58  M     ATA              -0.692    -0.538 Normal      0.313   \n 4 -0.505 M     ASY               0.427    -0.538 Normal      0.117   \n 5 -1.26  F     NAP              -0.972    -0.538 ST          0.000107\n 6 -1.91  M     ATA              -0.692    -0.538 Normal      0.900   \n 7  0.250 M     NAP              -0.133    -0.538 Normal     -0.900   \n 8 -1.37  M     ASY              -0.133    -0.538 ST         -0.274   \n 9 -2.34  M     ATA              -0.413    -0.538 Normal      0.704   \n10 -0.613 F     ASY               0.987    -0.538 Normal     -0.274   \n# ℹ 173 more rows\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\nFinally, the model will be run using the caret package, with the same specifications as for KNN, with the exception of a tuning parameter. Here, we use the method “glm” and specify “binomial” for family since we have a binary response for heart disease.\n\nheartTrainControl1 &lt;- trainControl(method = \"repeatedcv\", number = 10, \n                                  repeats = 3)\nset.seed(100)\nglmFit1 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed, method = \"glm\",\n                trControl = heartTrainControl1, family = \"binomial\")\nglmFit1$results\n\n  parameter Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.814237 0.6233833 0.04835864 0.0989383\n\n\nThe first model has a prediction accuracy of 81.42%. For the second model, RestingBP will be dropped from the transformed data.\n\nheartTrainTransformed2 &lt;- heartTrainTransformed |&gt;\n  select(-RestingBP)\nheartTestTransformed2 &lt;- heartTestTransformed |&gt;\n  select(-RestingBP)\n\nNow the model 2 can be fit as seen below. In looking at the fit results, we can see that dropping the RestingBP variable did not improve the accuracy, which now sits slightly lower at 81.38%\n\nheartTrainControl2 &lt;- trainControl(method = \"repeatedcv\", number = 10, \n                                  repeats = 3)\nset.seed(100)\nglmFit2 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed2, method = \"glm\",\n                trControl = heartTrainControl2, family = \"binomial\")\nglmFit2$results\n\n  parameter  Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.8137552 0.6227338 0.04991677 0.1016742\n\n\nFor model 3, the RestingECG variable was removed from the transformed train and test sets and the model rerun. This model has an accuracy of 81.15%, which is lower than the previous two models.\n\nheartTrainTransformed3 &lt;- heartTrainTransformed2 |&gt;\n  select(-RestingECG)\nheartTestTransformed3 &lt;- heartTestTransformed2 |&gt;\n  select(-RestingECG)\nheartTrainControl3 &lt;- trainControl(method = \"repeatedcv\", number = 10, \n                                  repeats = 3)\nset.seed(100)\nglmFit3 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed3, method = \"glm\",\n                trControl = heartTrainControl3, family = \"binomial\")\nglmFit3$results\n\n  parameter  Accuracy     Kappa AccuracySD   KappaSD\n1      none 0.8114719 0.6179063 0.05034342 0.1026898\n\n\nOut of all 3 models, the first one with the Cholesterol variable removed is the most accurate, with an accuracy rate of 81.42%. Accuracy on the test set is shown below using confusionMatrix(). The accuracy on the test data set is 81.97%. Interestingly, this exactly the same as the accuracy for the KNN model presented previously.\n\nconfusionMatrix(data = heartTestTransformed$HeartDisease, \n                reference = predict(glmFit1, newdata = heartTestTransformed))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 65 17\n         1 16 85\n                                          \n               Accuracy : 0.8197          \n                 95% CI : (0.7562, 0.8725)\n    No Information Rate : 0.5574          \n    P-Value [Acc &gt; NIR] : 5.843e-14       \n                                          \n                  Kappa : 0.635           \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.8025          \n            Specificity : 0.8333          \n         Pos Pred Value : 0.7927          \n         Neg Pred Value : 0.8416          \n             Prevalence : 0.4426          \n         Detection Rate : 0.3552          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8179          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "ST558HW5.html#tree-models",
    "href": "ST558HW5.html#tree-models",
    "title": "ST558 Homework 5",
    "section": "Tree Models",
    "text": "Tree Models\nNow, we want to fit the data from the 3 posited models in the logistic regression section to various types of tree models. Since the same 3 models used in logistic regression will be used here, the same training and test sets from the logistic regression section will be used here.\n\nClassification Tree\nThe code and output for model 1 where the Cholesterol variable is removed is presented below. Since this is model 1, we will use heartTrainControl1 from the logistic regression section, which used repeatedcv, 10 folds and 3 repeats. The best model for model 1 is the one with a complexity parameter of 0.004, with an accuracy of 79.65%.\n\nset.seed(100)\ntreeFit1 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed, \n                  method = \"rpart\",\n                  trControl = heartTrainControl1, \n                  tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit1$results\n\n       cp  Accuracy     Kappa AccuracySD    KappaSD\n1   0.000 0.7869805 0.5674633 0.04593100 0.09380032\n2   0.001 0.7923859 0.5778321 0.04644381 0.09482932\n3   0.002 0.7923859 0.5773961 0.04644381 0.09500933\n4   0.003 0.7932930 0.5790846 0.04678119 0.09567766\n5   0.004 0.7960515 0.5840635 0.04706789 0.09668206\n6   0.005 0.7942250 0.5805555 0.04653982 0.09562957\n7   0.006 0.7924107 0.5774316 0.04712529 0.09635247\n8   0.007 0.7819945 0.5564815 0.04608977 0.09369019\n9   0.008 0.7824575 0.5578582 0.04708382 0.09507559\n10  0.009 0.7838088 0.5605707 0.04690218 0.09510793\n11  0.010 0.7833522 0.5604414 0.04421452 0.08913738\n12  0.011 0.7887761 0.5711604 0.04046531 0.08087986\n13  0.012 0.7896708 0.5733335 0.03970507 0.07962357\n14  0.013 0.7896708 0.5736162 0.03970507 0.07974504\n15  0.014 0.7933053 0.5812763 0.04105578 0.08218375\n16  0.015 0.7946752 0.5844556 0.04130972 0.08269487\n17  0.016 0.7946752 0.5844556 0.04130972 0.08269487\n18  0.017 0.7946752 0.5844556 0.04130972 0.08269487\n19  0.018 0.7946752 0.5844556 0.04130972 0.08269487\n20  0.019 0.7946752 0.5844556 0.04130972 0.08269487\n21  0.020 0.7946752 0.5844556 0.04130972 0.08269487\n22  0.021 0.7932863 0.5818885 0.04257838 0.08514344\n23  0.022 0.7932863 0.5818885 0.04257838 0.08514344\n24  0.023 0.7923854 0.5802959 0.04248062 0.08504177\n25  0.024 0.7919287 0.5794062 0.04237805 0.08485087\n26  0.025 0.7919287 0.5794062 0.04237805 0.08485087\n27  0.026 0.7919287 0.5794062 0.04237805 0.08485087\n28  0.027 0.7919287 0.5794062 0.04237805 0.08485087\n29  0.028 0.7923854 0.5804104 0.04323552 0.08671784\n30  0.029 0.7923854 0.5804104 0.04323552 0.08671784\n31  0.030 0.7923854 0.5804104 0.04323552 0.08671784\n32  0.031 0.7923854 0.5804104 0.04323552 0.08671784\n33  0.032 0.7923854 0.5804104 0.04323552 0.08671784\n34  0.033 0.7923854 0.5804104 0.04323552 0.08671784\n35  0.034 0.7896642 0.5753069 0.04595731 0.09170107\n36  0.035 0.7896642 0.5753069 0.04595731 0.09170107\n37  0.036 0.7896642 0.5753069 0.04595731 0.09170107\n38  0.037 0.7896642 0.5753069 0.04595731 0.09170107\n39  0.038 0.7860359 0.5686670 0.04880446 0.09701877\n40  0.039 0.7860359 0.5686670 0.04880446 0.09701877\n41  0.040 0.7860359 0.5686670 0.04880446 0.09701877\n42  0.041 0.7824012 0.5622764 0.04688279 0.09331750\n43  0.042 0.7824012 0.5622764 0.04688279 0.09331750\n44  0.043 0.7824012 0.5622764 0.04688279 0.09331750\n45  0.044 0.7824012 0.5622764 0.04688279 0.09331750\n46  0.045 0.7824012 0.5624403 0.04688279 0.09331462\n47  0.046 0.7824012 0.5624403 0.04688279 0.09331462\n48  0.047 0.7824012 0.5624403 0.04688279 0.09331462\n49  0.048 0.7846597 0.5675788 0.04801399 0.09579789\n50  0.049 0.7846597 0.5675788 0.04801399 0.09579789\n51  0.050 0.7846597 0.5675788 0.04801399 0.09579789\n52  0.051 0.7846597 0.5675788 0.04801399 0.09579789\n53  0.052 0.7846597 0.5675788 0.04801399 0.09579789\n54  0.053 0.7846597 0.5675788 0.04801399 0.09579789\n55  0.054 0.7846597 0.5675788 0.04801399 0.09579789\n56  0.055 0.7846597 0.5675788 0.04801399 0.09579789\n57  0.056 0.7846597 0.5675788 0.04801399 0.09579789\n58  0.057 0.7846597 0.5675788 0.04801399 0.09579789\n59  0.058 0.7846597 0.5675788 0.04801399 0.09579789\n60  0.059 0.7846597 0.5675788 0.04801399 0.09579789\n61  0.060 0.7846597 0.5675788 0.04801399 0.09579789\n62  0.061 0.7846597 0.5675788 0.04801399 0.09579789\n63  0.062 0.7846597 0.5675788 0.04801399 0.09579789\n64  0.063 0.7846597 0.5675788 0.04801399 0.09579789\n65  0.064 0.7846597 0.5675788 0.04801399 0.09579789\n66  0.065 0.7846597 0.5675788 0.04801399 0.09579789\n67  0.066 0.7846597 0.5675788 0.04801399 0.09579789\n68  0.067 0.7846597 0.5675788 0.04801399 0.09579789\n69  0.068 0.7846597 0.5675788 0.04801399 0.09579789\n70  0.069 0.7846597 0.5675788 0.04801399 0.09579789\n71  0.070 0.7846597 0.5675788 0.04801399 0.09579789\n72  0.071 0.7846597 0.5675788 0.04801399 0.09579789\n73  0.072 0.7846597 0.5675788 0.04801399 0.09579789\n74  0.073 0.7846597 0.5675788 0.04801399 0.09579789\n75  0.074 0.7846597 0.5675788 0.04801399 0.09579789\n76  0.075 0.7846597 0.5675788 0.04801399 0.09579789\n77  0.076 0.7846597 0.5675788 0.04801399 0.09579789\n78  0.077 0.7846597 0.5675788 0.04801399 0.09579789\n79  0.078 0.7846597 0.5675788 0.04801399 0.09579789\n80  0.079 0.7846597 0.5675788 0.04801399 0.09579789\n81  0.080 0.7846597 0.5675788 0.04801399 0.09579789\n82  0.081 0.7846597 0.5675788 0.04801399 0.09579789\n83  0.082 0.7846597 0.5675788 0.04801399 0.09579789\n84  0.083 0.7846597 0.5675788 0.04801399 0.09579789\n85  0.084 0.7846597 0.5675788 0.04801399 0.09579789\n86  0.085 0.7846597 0.5675788 0.04801399 0.09579789\n87  0.086 0.7846597 0.5675788 0.04801399 0.09579789\n88  0.087 0.7846597 0.5675788 0.04801399 0.09579789\n89  0.088 0.7846597 0.5675788 0.04801399 0.09579789\n90  0.089 0.7846597 0.5675788 0.04801399 0.09579789\n91  0.090 0.7846597 0.5675788 0.04801399 0.09579789\n92  0.091 0.7846597 0.5675788 0.04801399 0.09579789\n93  0.092 0.7846597 0.5675788 0.04801399 0.09579789\n94  0.093 0.7846597 0.5675788 0.04801399 0.09579789\n95  0.094 0.7846597 0.5675788 0.04801399 0.09579789\n96  0.095 0.7846597 0.5675788 0.04801399 0.09579789\n97  0.096 0.7846597 0.5675788 0.04801399 0.09579789\n98  0.097 0.7846597 0.5675788 0.04801399 0.09579789\n99  0.098 0.7846597 0.5675788 0.04801399 0.09579789\n100 0.099 0.7846597 0.5675788 0.04801399 0.09579789\n101 0.100 0.7846597 0.5675788 0.04801399 0.09579789\n\ntreeFit1$bestTune\n\n     cp\n5 0.004\n\n\nNow the same steps will be performed for model 2. The best model here is the one with a cp of 0.003m which has an accuracy of 80.20%.\n\nset.seed(100)\ntreeFit2 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed2, \n                  method = \"rpart\",\n                  trControl = heartTrainControl, \n                  tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit2$results\n\n       cp  Accuracy     Kappa AccuracySD    KappaSD\n1   0.000 0.7974217 0.5870229 0.04278024 0.08798146\n2   0.001 0.7974217 0.5870229 0.04278024 0.08798146\n3   0.002 0.8015066 0.5949823 0.04187390 0.08613788\n4   0.003 0.8019571 0.5958583 0.04174917 0.08590160\n5   0.004 0.7960579 0.5838254 0.04261605 0.08745106\n6   0.005 0.7915287 0.5752893 0.04263313 0.08661325\n7   0.006 0.7856230 0.5638542 0.04633216 0.09408682\n8   0.007 0.7770087 0.5462400 0.04435446 0.09011417\n9   0.008 0.7770150 0.5465962 0.04387992 0.08877674\n10  0.009 0.7801744 0.5531536 0.04357382 0.08858365\n11  0.010 0.7824389 0.5583837 0.04107395 0.08307212\n12  0.011 0.7892327 0.5721716 0.04092684 0.08165643\n13  0.012 0.7887576 0.5712730 0.03957862 0.07964703\n14  0.013 0.7896708 0.5736162 0.03970507 0.07974504\n15  0.014 0.7933053 0.5812763 0.04105578 0.08218375\n16  0.015 0.7946752 0.5844556 0.04130972 0.08269487\n17  0.016 0.7946752 0.5844556 0.04130972 0.08269487\n18  0.017 0.7946752 0.5844556 0.04130972 0.08269487\n19  0.018 0.7946752 0.5844556 0.04130972 0.08269487\n20  0.019 0.7946752 0.5844556 0.04130972 0.08269487\n21  0.020 0.7946752 0.5844556 0.04130972 0.08269487\n22  0.021 0.7932863 0.5818885 0.04257838 0.08514344\n23  0.022 0.7932863 0.5818885 0.04257838 0.08514344\n24  0.023 0.7923854 0.5802959 0.04248062 0.08504177\n25  0.024 0.7919287 0.5794062 0.04237805 0.08485087\n26  0.025 0.7919287 0.5794062 0.04237805 0.08485087\n27  0.026 0.7919287 0.5794062 0.04237805 0.08485087\n28  0.027 0.7919287 0.5794062 0.04237805 0.08485087\n29  0.028 0.7923854 0.5804104 0.04323552 0.08671784\n30  0.029 0.7923854 0.5804104 0.04323552 0.08671784\n31  0.030 0.7923854 0.5804104 0.04323552 0.08671784\n32  0.031 0.7923854 0.5804104 0.04323552 0.08671784\n33  0.032 0.7923854 0.5804104 0.04323552 0.08671784\n34  0.033 0.7923854 0.5804104 0.04323552 0.08671784\n35  0.034 0.7896642 0.5753069 0.04595731 0.09170107\n36  0.035 0.7896642 0.5753069 0.04595731 0.09170107\n37  0.036 0.7896642 0.5753069 0.04595731 0.09170107\n38  0.037 0.7896642 0.5753069 0.04595731 0.09170107\n39  0.038 0.7860359 0.5686670 0.04880446 0.09701877\n40  0.039 0.7860359 0.5686670 0.04880446 0.09701877\n41  0.040 0.7860359 0.5686670 0.04880446 0.09701877\n42  0.041 0.7824012 0.5622764 0.04688279 0.09331750\n43  0.042 0.7824012 0.5622764 0.04688279 0.09331750\n44  0.043 0.7824012 0.5622764 0.04688279 0.09331750\n45  0.044 0.7824012 0.5622764 0.04688279 0.09331750\n46  0.045 0.7824012 0.5624403 0.04688279 0.09331462\n47  0.046 0.7824012 0.5624403 0.04688279 0.09331462\n48  0.047 0.7824012 0.5624403 0.04688279 0.09331462\n49  0.048 0.7846597 0.5675788 0.04801399 0.09579789\n50  0.049 0.7846597 0.5675788 0.04801399 0.09579789\n51  0.050 0.7846597 0.5675788 0.04801399 0.09579789\n52  0.051 0.7846597 0.5675788 0.04801399 0.09579789\n53  0.052 0.7846597 0.5675788 0.04801399 0.09579789\n54  0.053 0.7846597 0.5675788 0.04801399 0.09579789\n55  0.054 0.7846597 0.5675788 0.04801399 0.09579789\n56  0.055 0.7846597 0.5675788 0.04801399 0.09579789\n57  0.056 0.7846597 0.5675788 0.04801399 0.09579789\n58  0.057 0.7846597 0.5675788 0.04801399 0.09579789\n59  0.058 0.7846597 0.5675788 0.04801399 0.09579789\n60  0.059 0.7846597 0.5675788 0.04801399 0.09579789\n61  0.060 0.7846597 0.5675788 0.04801399 0.09579789\n62  0.061 0.7846597 0.5675788 0.04801399 0.09579789\n63  0.062 0.7846597 0.5675788 0.04801399 0.09579789\n64  0.063 0.7846597 0.5675788 0.04801399 0.09579789\n65  0.064 0.7846597 0.5675788 0.04801399 0.09579789\n66  0.065 0.7846597 0.5675788 0.04801399 0.09579789\n67  0.066 0.7846597 0.5675788 0.04801399 0.09579789\n68  0.067 0.7846597 0.5675788 0.04801399 0.09579789\n69  0.068 0.7846597 0.5675788 0.04801399 0.09579789\n70  0.069 0.7846597 0.5675788 0.04801399 0.09579789\n71  0.070 0.7846597 0.5675788 0.04801399 0.09579789\n72  0.071 0.7846597 0.5675788 0.04801399 0.09579789\n73  0.072 0.7846597 0.5675788 0.04801399 0.09579789\n74  0.073 0.7846597 0.5675788 0.04801399 0.09579789\n75  0.074 0.7846597 0.5675788 0.04801399 0.09579789\n76  0.075 0.7846597 0.5675788 0.04801399 0.09579789\n77  0.076 0.7846597 0.5675788 0.04801399 0.09579789\n78  0.077 0.7846597 0.5675788 0.04801399 0.09579789\n79  0.078 0.7846597 0.5675788 0.04801399 0.09579789\n80  0.079 0.7846597 0.5675788 0.04801399 0.09579789\n81  0.080 0.7846597 0.5675788 0.04801399 0.09579789\n82  0.081 0.7846597 0.5675788 0.04801399 0.09579789\n83  0.082 0.7846597 0.5675788 0.04801399 0.09579789\n84  0.083 0.7846597 0.5675788 0.04801399 0.09579789\n85  0.084 0.7846597 0.5675788 0.04801399 0.09579789\n86  0.085 0.7846597 0.5675788 0.04801399 0.09579789\n87  0.086 0.7846597 0.5675788 0.04801399 0.09579789\n88  0.087 0.7846597 0.5675788 0.04801399 0.09579789\n89  0.088 0.7846597 0.5675788 0.04801399 0.09579789\n90  0.089 0.7846597 0.5675788 0.04801399 0.09579789\n91  0.090 0.7846597 0.5675788 0.04801399 0.09579789\n92  0.091 0.7846597 0.5675788 0.04801399 0.09579789\n93  0.092 0.7846597 0.5675788 0.04801399 0.09579789\n94  0.093 0.7846597 0.5675788 0.04801399 0.09579789\n95  0.094 0.7846597 0.5675788 0.04801399 0.09579789\n96  0.095 0.7846597 0.5675788 0.04801399 0.09579789\n97  0.096 0.7846597 0.5675788 0.04801399 0.09579789\n98  0.097 0.7846597 0.5675788 0.04801399 0.09579789\n99  0.098 0.7846597 0.5675788 0.04801399 0.09579789\n100 0.099 0.7846597 0.5675788 0.04801399 0.09579789\n101 0.100 0.7846597 0.5675788 0.04801399 0.09579789\n\ntreeFit2$bestTune\n\n     cp\n4 0.003\n\n\nNext is model 3, which excludes Cholesterol, RestingBP and RestingECG variables from the data. The model with the best fit had a cp of 0.003 and an accuracy of 79.97%.\n\nset.seed(100)\ntreeFit2 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed2, \n                  method = \"rpart\",\n                  trControl = heartTrainControl2, \n                  tuneGrid = expand.grid(cp = seq(0, 0.1, 0.001)))\ntreeFit2$results\n\n       cp  Accuracy     Kappa AccuracySD    KappaSD\n1   0.000 0.7974217 0.5870229 0.04278024 0.08798146\n2   0.001 0.7974217 0.5870229 0.04278024 0.08798146\n3   0.002 0.8015066 0.5949823 0.04187390 0.08613788\n4   0.003 0.8019571 0.5958583 0.04174917 0.08590160\n5   0.004 0.7960579 0.5838254 0.04261605 0.08745106\n6   0.005 0.7915287 0.5752893 0.04263313 0.08661325\n7   0.006 0.7856230 0.5638542 0.04633216 0.09408682\n8   0.007 0.7770087 0.5462400 0.04435446 0.09011417\n9   0.008 0.7770150 0.5465962 0.04387992 0.08877674\n10  0.009 0.7801744 0.5531536 0.04357382 0.08858365\n11  0.010 0.7824389 0.5583837 0.04107395 0.08307212\n12  0.011 0.7892327 0.5721716 0.04092684 0.08165643\n13  0.012 0.7887576 0.5712730 0.03957862 0.07964703\n14  0.013 0.7896708 0.5736162 0.03970507 0.07974504\n15  0.014 0.7933053 0.5812763 0.04105578 0.08218375\n16  0.015 0.7946752 0.5844556 0.04130972 0.08269487\n17  0.016 0.7946752 0.5844556 0.04130972 0.08269487\n18  0.017 0.7946752 0.5844556 0.04130972 0.08269487\n19  0.018 0.7946752 0.5844556 0.04130972 0.08269487\n20  0.019 0.7946752 0.5844556 0.04130972 0.08269487\n21  0.020 0.7946752 0.5844556 0.04130972 0.08269487\n22  0.021 0.7932863 0.5818885 0.04257838 0.08514344\n23  0.022 0.7932863 0.5818885 0.04257838 0.08514344\n24  0.023 0.7923854 0.5802959 0.04248062 0.08504177\n25  0.024 0.7919287 0.5794062 0.04237805 0.08485087\n26  0.025 0.7919287 0.5794062 0.04237805 0.08485087\n27  0.026 0.7919287 0.5794062 0.04237805 0.08485087\n28  0.027 0.7919287 0.5794062 0.04237805 0.08485087\n29  0.028 0.7923854 0.5804104 0.04323552 0.08671784\n30  0.029 0.7923854 0.5804104 0.04323552 0.08671784\n31  0.030 0.7923854 0.5804104 0.04323552 0.08671784\n32  0.031 0.7923854 0.5804104 0.04323552 0.08671784\n33  0.032 0.7923854 0.5804104 0.04323552 0.08671784\n34  0.033 0.7923854 0.5804104 0.04323552 0.08671784\n35  0.034 0.7896642 0.5753069 0.04595731 0.09170107\n36  0.035 0.7896642 0.5753069 0.04595731 0.09170107\n37  0.036 0.7896642 0.5753069 0.04595731 0.09170107\n38  0.037 0.7896642 0.5753069 0.04595731 0.09170107\n39  0.038 0.7860359 0.5686670 0.04880446 0.09701877\n40  0.039 0.7860359 0.5686670 0.04880446 0.09701877\n41  0.040 0.7860359 0.5686670 0.04880446 0.09701877\n42  0.041 0.7824012 0.5622764 0.04688279 0.09331750\n43  0.042 0.7824012 0.5622764 0.04688279 0.09331750\n44  0.043 0.7824012 0.5622764 0.04688279 0.09331750\n45  0.044 0.7824012 0.5622764 0.04688279 0.09331750\n46  0.045 0.7824012 0.5624403 0.04688279 0.09331462\n47  0.046 0.7824012 0.5624403 0.04688279 0.09331462\n48  0.047 0.7824012 0.5624403 0.04688279 0.09331462\n49  0.048 0.7846597 0.5675788 0.04801399 0.09579789\n50  0.049 0.7846597 0.5675788 0.04801399 0.09579789\n51  0.050 0.7846597 0.5675788 0.04801399 0.09579789\n52  0.051 0.7846597 0.5675788 0.04801399 0.09579789\n53  0.052 0.7846597 0.5675788 0.04801399 0.09579789\n54  0.053 0.7846597 0.5675788 0.04801399 0.09579789\n55  0.054 0.7846597 0.5675788 0.04801399 0.09579789\n56  0.055 0.7846597 0.5675788 0.04801399 0.09579789\n57  0.056 0.7846597 0.5675788 0.04801399 0.09579789\n58  0.057 0.7846597 0.5675788 0.04801399 0.09579789\n59  0.058 0.7846597 0.5675788 0.04801399 0.09579789\n60  0.059 0.7846597 0.5675788 0.04801399 0.09579789\n61  0.060 0.7846597 0.5675788 0.04801399 0.09579789\n62  0.061 0.7846597 0.5675788 0.04801399 0.09579789\n63  0.062 0.7846597 0.5675788 0.04801399 0.09579789\n64  0.063 0.7846597 0.5675788 0.04801399 0.09579789\n65  0.064 0.7846597 0.5675788 0.04801399 0.09579789\n66  0.065 0.7846597 0.5675788 0.04801399 0.09579789\n67  0.066 0.7846597 0.5675788 0.04801399 0.09579789\n68  0.067 0.7846597 0.5675788 0.04801399 0.09579789\n69  0.068 0.7846597 0.5675788 0.04801399 0.09579789\n70  0.069 0.7846597 0.5675788 0.04801399 0.09579789\n71  0.070 0.7846597 0.5675788 0.04801399 0.09579789\n72  0.071 0.7846597 0.5675788 0.04801399 0.09579789\n73  0.072 0.7846597 0.5675788 0.04801399 0.09579789\n74  0.073 0.7846597 0.5675788 0.04801399 0.09579789\n75  0.074 0.7846597 0.5675788 0.04801399 0.09579789\n76  0.075 0.7846597 0.5675788 0.04801399 0.09579789\n77  0.076 0.7846597 0.5675788 0.04801399 0.09579789\n78  0.077 0.7846597 0.5675788 0.04801399 0.09579789\n79  0.078 0.7846597 0.5675788 0.04801399 0.09579789\n80  0.079 0.7846597 0.5675788 0.04801399 0.09579789\n81  0.080 0.7846597 0.5675788 0.04801399 0.09579789\n82  0.081 0.7846597 0.5675788 0.04801399 0.09579789\n83  0.082 0.7846597 0.5675788 0.04801399 0.09579789\n84  0.083 0.7846597 0.5675788 0.04801399 0.09579789\n85  0.084 0.7846597 0.5675788 0.04801399 0.09579789\n86  0.085 0.7846597 0.5675788 0.04801399 0.09579789\n87  0.086 0.7846597 0.5675788 0.04801399 0.09579789\n88  0.087 0.7846597 0.5675788 0.04801399 0.09579789\n89  0.088 0.7846597 0.5675788 0.04801399 0.09579789\n90  0.089 0.7846597 0.5675788 0.04801399 0.09579789\n91  0.090 0.7846597 0.5675788 0.04801399 0.09579789\n92  0.091 0.7846597 0.5675788 0.04801399 0.09579789\n93  0.092 0.7846597 0.5675788 0.04801399 0.09579789\n94  0.093 0.7846597 0.5675788 0.04801399 0.09579789\n95  0.094 0.7846597 0.5675788 0.04801399 0.09579789\n96  0.095 0.7846597 0.5675788 0.04801399 0.09579789\n97  0.096 0.7846597 0.5675788 0.04801399 0.09579789\n98  0.097 0.7846597 0.5675788 0.04801399 0.09579789\n99  0.098 0.7846597 0.5675788 0.04801399 0.09579789\n100 0.099 0.7846597 0.5675788 0.04801399 0.09579789\n101 0.100 0.7846597 0.5675788 0.04801399 0.09579789\n\ntreeFit2$bestTune\n\n     cp\n4 0.003\n\n\nOf the 3 models, model 2 had the best accuracy at 80.20%. The confusion matrix output is presented below. The accuracy using test data is 83.61%.\n\nconfusionMatrix(data = heartTestTransformed2$HeartDisease, \n                reference = predict(treeFit2, newdata = heartTestTransformed2))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 63 19\n         1 11 90\n                                          \n               Accuracy : 0.8361          \n                 95% CI : (0.7743, 0.8866)\n    No Information Rate : 0.5956          \n    P-Value [Acc &gt; NIR] : 1.888e-12       \n                                          \n                  Kappa : 0.6655          \n                                          \n Mcnemar's Test P-Value : 0.2012          \n                                          \n            Sensitivity : 0.8514          \n            Specificity : 0.8257          \n         Pos Pred Value : 0.7683          \n         Neg Pred Value : 0.8911          \n             Prevalence : 0.4044          \n         Detection Rate : 0.3443          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8385          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nRandom Forest\nThe code and output for model 1 where the Cholesterol variable is removed is presented below. Since this is model 1, we will use heartTrainControl1 from the previous two sections, which used repeatedcv (cross validation), 10 folds and 3 repeats. Since this is for a random forest, we use “rf” for method. The tuning parameter is mtry, which specifies the number of predictors in the data to be randomly sampled at each split. For model 1, we have 10 predictors, so mtry is a data frame of 1:10. The best model for model 1 is the one with an mtry of 2 variables, with an accuracy of 82.52%.\n\nset.seed(100)\nrfFit1 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed, \n                  method = \"rf\",\n                  trControl = heartTrainControl1, \n                  tuneGrid = data.frame(mtry = 1:10))\nrfFit1$results\n\n   mtry  Accuracy     Kappa AccuracySD    KappaSD\n1     1 0.8133169 0.6205232 0.04080770 0.08362225\n2     2 0.8251645 0.6466972 0.04208647 0.08484608\n3     3 0.8233691 0.6419224 0.03580293 0.07232984\n4     4 0.8178584 0.6309967 0.03455927 0.06908260\n5     5 0.8201599 0.6353787 0.03477977 0.06995916\n6     6 0.8160818 0.6271489 0.03475278 0.07000140\n7     7 0.8115149 0.6180277 0.03464980 0.06905289\n8     8 0.8101638 0.6154270 0.03341898 0.06686043\n9     9 0.8065046 0.6076599 0.03510831 0.07037622\n10   10 0.8001423 0.5951327 0.03515418 0.07068520\n\nrfFit1$bestTune\n\n  mtry\n2    2\n\n\nThe fit for model 2, with mtry of 9, since we are dropping the RestingBP variable for this model, is presented below. A mtry of 2 was also selected for this model, with an accuracy of 82.06%.\n\nset.seed(100)\nrfFit2 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed2, \n                  method = \"rf\",\n                  trControl = heartTrainControl2, \n                  tuneGrid = data.frame(mtry = 1:9))\nrfFit2$results\n\n  mtry  Accuracy     Kappa AccuracySD    KappaSD\n1    1 0.8174646 0.6291965 0.04363995 0.09005835\n2    2 0.8206232 0.6382218 0.04381688 0.08790541\n3    3 0.8124842 0.6209813 0.03929718 0.07837967\n4    4 0.8079053 0.6109714 0.04387788 0.08769356\n5    5 0.8056469 0.6063305 0.04269600 0.08602755\n6    6 0.8029197 0.6005207 0.04030779 0.08114116\n7    7 0.7970268 0.5887011 0.04253811 0.08546944\n8    8 0.7974462 0.5897933 0.03823099 0.07659990\n9    9 0.7983471 0.5912023 0.04248587 0.08488424\n\nrfFit2$bestTune\n\n  mtry\n2    2\n\n\nThe fit for model 3, with mtry of 8, since we are dropping the RestingECG variable for this model, is presented below. The model with an mtry of 1 gave the highest accuracy, which as 82.02%.\n\nset.seed(100)\nrfFit3 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed3, \n                  method = \"rf\",\n                  trControl = heartTrainControl3, \n                  tuneGrid = data.frame(mtry = 1:8))\nrfFit3$results\n\n  mtry  Accuracy     Kappa AccuracySD    KappaSD\n1    1 0.8201727 0.6362198 0.04853444 0.09879635\n2    2 0.8192780 0.6352948 0.04601381 0.09237274\n3    3 0.8120213 0.6196776 0.04311814 0.08620577\n4    4 0.8028947 0.6005093 0.04381189 0.08788677\n5    5 0.7969895 0.5884186 0.04151354 0.08393799\n6    6 0.7929107 0.5802062 0.04072806 0.08231423\n7    7 0.7851665 0.5643443 0.04004483 0.08078150\n8    8 0.7856293 0.5650128 0.04385776 0.08842402\n\nrfFit3$bestTune\n\n  mtry\n1    1\n\n\nOf the 3 random forest models above, the first model with all predictors except Cholesterol performed the best. The confusion matrix output is presented below for this model. The performance on the test data set was 83.61% accuracy.\n\nconfusionMatrix(data = heartTestTransformed$HeartDisease, \n                reference = predict(rfFit1, newdata = heartTestTransformed))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 64 18\n         1 12 89\n                                          \n               Accuracy : 0.8361          \n                 95% CI : (0.7743, 0.8866)\n    No Information Rate : 0.5847          \n    P-Value [Acc &gt; NIR] : 2.43e-13        \n                                          \n                  Kappa : 0.6663          \n                                          \n Mcnemar's Test P-Value : 0.3613          \n                                          \n            Sensitivity : 0.8421          \n            Specificity : 0.8318          \n         Pos Pred Value : 0.7805          \n         Neg Pred Value : 0.8812          \n             Prevalence : 0.4153          \n         Detection Rate : 0.3497          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8369          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nBoosted Tree\nThe same transformed sets used in the previous sections will be used here. For boosted trees, we will use gbm for the method. Since we want to test variations of tuning parameters, a grid with these parameters will be created using expand.grid and this data frame will then be used in tuneGrid. The best model used n.trees of 25 and an interaction depth of 3. The accuracy of this model is 82.33%.\n\ngrid &lt;- expand.grid(n.trees = c(25,50, 100, 200), \n                    interaction.depth = c(1:3), \n                    shrinkage = c(0.1), \n                    n.minobsinnode = c(10))\nset.seed(100)\nboostFit1 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed, \n                  method = \"gbm\",\n                  trControl = heartTrainControl1, \n                  tuneGrid = grid, verbose = FALSE)\nboostFit1$results\n\n   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa\n1        0.1                 1             10      25 0.7928727 0.5806417\n5        0.1                 2             10      25 0.8069234 0.6079759\n9        0.1                 3             10      25 0.8233257 0.6413758\n2        0.1                 1             10      50 0.8137237 0.6226437\n6        0.1                 2             10      50 0.8214934 0.6380441\n10       0.1                 3             10      50 0.8215613 0.6385052\n3        0.1                 1             10     100 0.8205673 0.6369787\n7        0.1                 2             10     100 0.8183084 0.6316129\n11       0.1                 3             10     100 0.8224435 0.6399704\n4        0.1                 1             10     200 0.8178770 0.6314330\n8        0.1                 2             10     200 0.8146741 0.6244337\n12       0.1                 3             10     200 0.8128851 0.6209737\n   AccuracySD    KappaSD\n1  0.04943801 0.10070863\n5  0.04995882 0.10192840\n9  0.04533459 0.09263861\n2  0.04787013 0.09835523\n6  0.04721135 0.09695721\n10 0.03881041 0.07925127\n3  0.04700795 0.09560616\n7  0.04919366 0.10042691\n11 0.03972635 0.08114557\n4  0.05021313 0.10260911\n8  0.04537719 0.09316380\n12 0.04167730 0.08487011\n\nboostFit1$bestTune\n\n  n.trees interaction.depth shrinkage n.minobsinnode\n9      25                 3       0.1             10\n\n\nThe output for model 2 is presented below. The optimal settings were for 100 trees and an interaction depth of 2. The accuracy of this model is 82.47%.\n\nset.seed(100)\nboostFit2 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed2, \n                  method = \"gbm\",\n                  trControl = heartTrainControl2, \n                  tuneGrid = grid, verbose = FALSE)\nboostFit2$results\n\n   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa\n1        0.1                 1             10      25 0.7928727 0.5806417\n5        0.1                 2             10      25 0.8092130 0.6127782\n9        0.1                 3             10      25 0.8187595 0.6321203\n2        0.1                 1             10      50 0.8132919 0.6218593\n6        0.1                 2             10      50 0.8187905 0.6325275\n10       0.1                 3             10      50 0.8192593 0.6334945\n3        0.1                 1             10     100 0.8224005 0.6405965\n7        0.1                 2             10     100 0.8246898 0.6443546\n11       0.1                 3             10     100 0.8174018 0.6296192\n4        0.1                 1             10     200 0.8224373 0.6406566\n8        0.1                 2             10     200 0.8137547 0.6219810\n12       0.1                 3             10     200 0.8165323 0.6274472\n   AccuracySD    KappaSD\n1  0.04943801 0.10070863\n5  0.05032154 0.10275455\n9  0.04583552 0.09360434\n2  0.04746243 0.09773845\n6  0.04488901 0.09207809\n10 0.03972563 0.08100211\n3  0.04314156 0.08797859\n7  0.04352733 0.08943376\n11 0.03883816 0.07929034\n4  0.04302682 0.08796330\n8  0.04127571 0.08402558\n12 0.03796807 0.07736901\n\nboostFit2$bestTune\n\n  n.trees interaction.depth shrinkage n.minobsinnode\n7     100                 2       0.1             10\n\n\nThe output for model 3 is presented below. The optimal settings were for 100 trees and an interaction depth of 2. The accuracy of this model is 82.56%.\n\nset.seed(100)\nboostFit3 &lt;- train(HeartDisease ~ ., data = heartTrainTransformed3, \n                  method = \"gbm\",\n                  trControl = heartTrainControl3, \n                  tuneGrid = grid, verbose = FALSE)\nboostFit3$results\n\n   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa\n1        0.1                 1             10      25 0.7928727 0.5806417\n5        0.1                 2             10      25 0.8096634 0.6137357\n9        0.1                 3             10      25 0.8192161 0.6329016\n2        0.1                 1             10      50 0.8132919 0.6218593\n6        0.1                 2             10      50 0.8183339 0.6316349\n10       0.1                 3             10      50 0.8206294 0.6364518\n3        0.1                 1             10     100 0.8228571 0.6414965\n7        0.1                 2             10     100 0.8256213 0.6468173\n11       0.1                 3             10     100 0.8197283 0.6342570\n4        0.1                 1             10     200 0.8229001 0.6416137\n8        0.1                 2             10     200 0.8124407 0.6193236\n12       0.1                 3             10     200 0.8142987 0.6221569\n   AccuracySD    KappaSD\n1  0.04943801 0.10070863\n5  0.05002051 0.10212444\n9  0.04649623 0.09512165\n2  0.04746243 0.09773845\n6  0.04434589 0.09101890\n10 0.03951869 0.08031833\n3  0.04305870 0.08780468\n7  0.04414476 0.09021264\n11 0.04123607 0.08455477\n4  0.04365448 0.08922363\n8  0.03836767 0.07827551\n12 0.04270759 0.08776276\n\nboostFit3$bestTune\n\n  n.trees interaction.depth shrinkage n.minobsinnode\n7     100                 2       0.1             10\n\n\nOf the 3 models, model 3 gave the best accuracy. The confusion matrix is below. The accuracy on the test data is 83.06%.\n\nconfusionMatrix(data = heartTestTransformed3$HeartDisease, \n                reference = predict(boostFit3, newdata = heartTestTransformed3))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 65 17\n         1 14 87\n                                          \n               Accuracy : 0.8306          \n                 95% CI : (0.7683, 0.8819)\n    No Information Rate : 0.5683          \n    P-Value [Acc &gt; NIR] : 3.715e-14       \n                                          \n                  Kappa : 0.6563          \n                                          \n Mcnemar's Test P-Value : 0.7194          \n                                          \n            Sensitivity : 0.8228          \n            Specificity : 0.8365          \n         Pos Pred Value : 0.7927          \n         Neg Pred Value : 0.8614          \n             Prevalence : 0.4317          \n         Detection Rate : 0.3552          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.8297          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "ST558HW5.html#wrap-up",
    "href": "ST558HW5.html#wrap-up",
    "title": "ST558 Homework 5",
    "section": "Wrap Up",
    "text": "Wrap Up\nKNN and the best performing logistic regression model both had an accuracy of 81.97% on the test data. Similarly, the best classification tree and random forest model had an accuracy of 83.61% on the test data. Lastly, the boosted tree had an accuracy of 83.06% for the test data. In conclusion, there was a tie between the classification tree and random forest models for best performer, with both having an accuracy of 83.61% on the test set. However, the random forest model with the best accuracy used 10 variables while the classification model used 9. Since we usually want to use the most parsimonious model, the classification model may be the better option of the two."
  }
]